{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c07fb252",
   "metadata": {},
   "source": [
    "# Misinformation proof of concept (POC) pipeline\n",
    "\n",
    "This notebook has several steps. Each of the steps will likely be replaced with something better as we learn more. @Bing: Please do the things marked \"Please...\" in the markdown cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f49b8de",
   "metadata": {},
   "source": [
    "## Step 1: Get data\n",
    "\n",
    "The data used in this POC is from https://covid.dh.miami.edu/get/ (English; Miami; November, 2020). It's good enough for the POC, but it's insufficient for what we need. Instead, we'll use the data in the [avax-tweets-dataset](https://github.com/gmuric/avax-tweets-dataset) -- described in the \"[COVID-19 Vaccine Hesitancy on Social Media: Building a Public Twitter Dataset of Anti-vaccine Content, Vaccine Misinformation and Conspiracies](https://arxiv.org/abs/2105.05134)\" paper. Since this collection (and there may be a temporaly component), we may also use the [COVID-19 Tweets](https://github.com/echen102/COVID-19-TweetIDs) corpus, which maintains an ongoing collection. This latter corpus only has the tweet IDs, not the actual tweets. We need to download this data.\n",
    "\n",
    "Please do the following:\n",
    "\n",
    "* Coordinate with Rakshith Venkatachalapathy, Ariana Jorgensen and Ganesh Venkata. (They are working on a similar project, so we can combine efforts and help each other.)\n",
    "* Get a Twitter API key via the [apply for access](https://developer.twitter.com/en/apply-for-access) link.\n",
    "* Download tweets using the `hydrate.py` script int the [COVID-19 Tweets](https://github.com/echen102/COVID-19-TweetIDs) repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6f8f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data we get from the COVID-19 Tweets repo will probably be in a different format, so we might need to adjust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d43ba275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in a report marked internal document plz keep ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jit dont you got covid19 URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#breaking miamidade mayor daniella levine cava...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@user @user delayed to wednesday URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 staff with covid19 at school dont know who t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18711</th>\n",
       "      <td>same people blaming trump for covid19 getting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18712</th>\n",
       "      <td>american medical association slams trumps clai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18713</th>\n",
       "      <td>study trump rallies may be responsible for an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18714</th>\n",
       "      <td>rain covid19 halloween definitely canceled all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18715</th>\n",
       "      <td>16 us states reported their highest oneday cor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18716 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "0      in a report marked internal document plz keep ...\n",
       "1                           jit dont you got covid19 URL\n",
       "2      #breaking miamidade mayor daniella levine cava...\n",
       "3                   @user @user delayed to wednesday URL\n",
       "4      2 staff with covid19 at school dont know who t...\n",
       "...                                                  ...\n",
       "18711  same people blaming trump for covid19 getting ...\n",
       "18712  american medical association slams trumps clai...\n",
       "18713  study trump rallies may be responsible for an ...\n",
       "18714  rain covid19 halloween definitely canceled all...\n",
       "18715  16 us states reported their highest oneday cor...\n",
       "\n",
       "[18716 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dhcovid_texts_month-2020-11_en_fl.txt', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a0f69",
   "metadata": {},
   "source": [
    "## Step 2: Filter the tweets\n",
    "\n",
    "In this POC example, I filtered for any tweets with negative sentiment using VADER in NLTK. I think it is fine to use valence for the POC. However, in the Misinformation project, we want tweets which produce the \"discrete emotion\" of fear around vaccines for those who read the tweet. There are existing approaches to finding discrete emotions, including in the paper \"[Joint Discrete and Continuous Emotion Prediction Using Ensemble and End-to-End Approaches](https://dl.acm.org/doi/10.1145/3242969.3242972).\"\n",
    "\n",
    "We should filter for vaccine-related tweets. The \"COVID-19 Vaccine Hesitancy on Social Media...\" paper specifically uses certain [keywords](https://github.com/gmuric/avax-tweets-dataset/blob/main/keywords.txt) as a filter. We should use the same keywords if we pull in data from the \"COVID-19 Tweets\" corpus. If we do not use these keywords, one potentially important missing part of this step is finding misinformation. (It's not good enough to find a tweet like, \"I hate covid\" because we don't care about someone's opinions per se. Instead, we'll want to get tweets similar to \"With #COVID19, all the toilet paper in the supermarket will be gone.\" In other words, we want tweets expressing statements which we can test for veracity... regargless of whether they are true or false.)\n",
    "\n",
    "Please:\n",
    "\n",
    "* Perform a literature review of discrete emotion and find the highest performing / state-of-the-art (SOTA) model\n",
    "* Perform a literature review on finding propositional phrases in text and find the highest performing / SOTA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf550ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to install NLTK for the below to work.\n",
    "# Instructions: https://www.guru99.com/download-install-nltk.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f120f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6365 negative tweets.\n"
     ]
    }
   ],
   "source": [
    "# Finding negative tweets with VADER\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "negative_tweets = []\n",
    "for index in range(len(df)):\n",
    "    if analyzer.polarity_scores(df.iloc[index, 0])['compound'] < 0:\n",
    "        negative_tweets.append(index)\n",
    "print('Found', len(negative_tweets), 'negative tweets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56a6b663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@user @user delayed to wednesday URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 staff with covid19 at school dont know who t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nigga said he went to the emergency room for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>chinese sociologist we are driving america to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>as seen on risk insurance magazine mask wearin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18703</th>\n",
       "      <td>my entire neighborhood is dark no one has porc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18706</th>\n",
       "      <td>i hate covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18708</th>\n",
       "      <td>mmcht i really want to go out but it wanna rai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18709</th>\n",
       "      <td>drop in reported cases from 5592 new cases rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18712</th>\n",
       "      <td>american medical association slams trumps clai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6365 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "3                   @user @user delayed to wednesday URL\n",
       "4      2 staff with covid19 at school dont know who t...\n",
       "12     nigga said he went to the emergency room for a...\n",
       "14     chinese sociologist we are driving america to ...\n",
       "16     as seen on risk insurance magazine mask wearin...\n",
       "...                                                  ...\n",
       "18703  my entire neighborhood is dark no one has porc...\n",
       "18706                                       i hate covid\n",
       "18708  mmcht i really want to go out but it wanna rai...\n",
       "18709  drop in reported cases from 5592 new cases rep...\n",
       "18712  american medical association slams trumps clai...\n",
       "\n",
       "[6365 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_df = df.iloc[negative_tweets]\n",
    "negative_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d037a0",
   "metadata": {},
   "source": [
    "## Step 3: Get topic models from tweets\n",
    "\n",
    "Of the tweets we filter for, we want to produce a number of topics. We can get topic models from gensim. I took this pipeline from [Topic Modeling (LDA)](https://elibooklover.github.io/Tutorials/Python/topicmodelingLDA/) (Hoyeol Kim) and made some modifications for the data above. I got some additional steps from [Topic Modeling in Python with NLTK and Gensim](https://datascienceplus.com/topic-modeling-in-python-with-nltk-and-gensim/) (Susan Li).\n",
    "\n",
    "Please extract the \"ideal\" number of topics. Sooraj Subrahmannian (a USF graduate!) has a [Medium post which includes this](https://medium.com/@soorajsubrahmannian/extracting-hidden-topics-in-a-corpus-55b2214fc17d), but there may be better approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c203425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In addition to the installations (gensim, etc.), you may need to `python3 -m spacy download en` for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b42ab6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in range(len(sentences)):\n",
    "        yield(gensim.utils.simple_preprocess(str(sentences.iloc[sentence,0]), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d78bde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = list(sent_to_words(negative_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba4db08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "# print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4bd56cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f363b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# The remainder of this cell is the LDA (genim) pipeline.\n",
    "# It might be useful to have it in one function, ala Susan Li's example, for Step 4, below.\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spaCy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spaCy download en\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Perform lemmatization keeping noun, adjective, verb, and adverb\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56060191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "# print(corpus[:1])\n",
    "# Also view:\n",
    "# id2word[0]\n",
    "# ... and:\n",
    "# [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e489553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bee683de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# The remainder of the cells in this step of the pipeline are not necessary, but may be useful\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c1957c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model for future use...\n",
    "lda_model.save('misinformation_model.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95b81b01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.047*\"take\" + 0.034*\"shit\" + 0.027*\"thing\" + 0.027*\"biden\" + 0.022*\"give\"')\n",
      "(1, '0.079*\"case\" + 0.078*\"trump\" + 0.057*\"vote\" + 0.056*\"death\" + 0.020*\"number\"')\n",
      "(2, '0.044*\"country\" + 0.025*\"warn\" + 0.025*\"fight\" + 0.023*\"rise\" + 0.018*\"top\"')\n",
      "(3, '0.043*\"still\" + 0.039*\"lose\" + 0.037*\"see\" + 0.032*\"want\" + 0.026*\"year\"')\n",
      "(4, '0.073*\"test\" + 0.042*\"even\" + 0.038*\"rally\" + 0.034*\"week\" + 0.030*\"fuck\"')\n",
      "(5, '0.044*\"stop\" + 0.038*\"due\" + 0.023*\"hurt\" + 0.018*\"sick\" + 0.018*\"good\"')\n",
      "(6, '0.063*\"mask\" + 0.043*\"risk\" + 0.043*\"virus\" + 0.028*\"believe\" + 0.026*\"lead\"')\n",
      "(7, '0.056*\"say\" + 0.041*\"die\" + 0.032*\"know\" + 0.032*\"day\" + 0.027*\"think\"')\n",
      "(8, '0.179*\"url\" + 0.145*\"covid\" + 0.038*\"user\" + 0.037*\"people\" + 0.033*\"get\"')\n",
      "(9, '0.000*\"understatement\" + 0.000*\"sc\" + 0.000*\"equally\" + 0.000*\"bespoke\" + 0.000*\"scarce\"')\n"
     ]
    }
   ],
   "source": [
    "# The model is difficult to understand, but we can force it to give us some breadcrumbs...\n",
    "\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dffa657",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -8.517655227357714\n",
      "\n",
      "Coherence Score:  0.3486245536440807\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation... potentially useful if we have more than one model.\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8224898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# Checkpoint: It might be nice to produce a chart here.\n",
    "# PyLDAVis is popular. I think a simple bar chart would be fine.\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38775a82",
   "metadata": {},
   "source": [
    "## Step 4: Getting the most representative tweets\n",
    "\n",
    "Once the model has been created, we will need to get a number (10-20?) tweets representing each topic. (I was wrong about not being able to get the most repreentative tweets from a model. It's reasonably easy since we can get a probabliity distribution of a tweet over the list of topics.)\n",
    "\n",
    "Please:\n",
    "\n",
    "* Get the probablity distribution of the original corpus -- by passing the original corpus back through the model?\n",
    "* With `X` as an int parameter, extract the top `X` tweets per topic -- i.e. the tweets with the highest probability of belonging to a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fb5166a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.06613438), (1, 0.09010554), (2, 0.09333344), (3, 0.084754035), (4, 0.05787435), (5, 0.07318368), (6, 0.038647328), (7, 0.15742238), (8, 0.33793208)]\n",
      "[(0, 0.10725417), (1, 0.09008495), (2, 0.052161235), (3, 0.12588437), (4, 0.057832368), (5, 0.0731479), (6, 0.038617186), (7, 0.11624437), (8, 0.3381602)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "new_doc = ['instead of herd immunity, we have achieved herd stupidity', 'The real pandemic is FEAR']\n",
    "\n",
    "data_words_nostops = remove_stopwords(new_doc)\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "corpus = [id2word.doc2bow(text) for text in data_lemmatized]\n",
    "\n",
    "topics = lda_model.get_document_topics(corpus)\n",
    "\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "       \n",
    "# new_doc_tfidf = [id2word.doc2bow(corpus)]\n",
    "# new_doc_tfidf\n",
    "# \n",
    "# \n",
    "# new_doc_tfidf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
